#### 1. 그래프 신경망 복습

1.1 귀납식 정점 표현 학습

- 정점을 임베딩하는 함수, 즉 인코더를 학습하는 귀납식 정점 표현 학습은 여러 장점을 갖습니다
  1) 학습이 진행된 이후에 추가된 정점에 대해서도 임베딩을 얻을 수 있습니다
  2) 모든 정점에 대한 임베딩을 미리 계산하여 저장해둘 필요가 없습니다
  3) 정점이 속성(Attribute) 정보를 가진 경우에 이를 활용할 수 있습니다
- 그래프 신경망(Graph Neural Network)은 대표적인 귀납식 임베딩 방법입니다

1.2 그래프 신경망의 구조

- 그래프 신경망은 이웃 정점들의 정보를 집계하는 과정을 반복하여 임베딩을 얻습니다
- 집계 함수의 형태에 따라, 그래프 신경망, 그래프 합성곱 신경망, GraphSAGE 등이 구분됩니다
- 그래프 신경망은 비지도 학습, 지도 학습이 모두 가능합니다
- 비지도 학습에서는 정점간 거리를 “보존”하는 것을 목표로 합니다
- 지도 학습에서는 후속 과제의 손실함수를 이용해 종단종 학습을 합니다

1.3 그래프 신경망의 활용

- 학습된 신경망을 적용하여, 학습에 사용되지 않은 정점, 학습 이후에 추가된 정점, 심지어 새로운 그래프의 정점의 임베딩을 얻을 수 있습니다

#### 2. 그래프 신경망에서의 어텐션

2.1 기본 그래프 신경망의 한계

- 기본 그래프 신경망에서는 이웃들의 정보를 동일한 가중치로 평균을 냅니다
- 그래프 합성곱 신경망에서 역시 단순히 연결성을 고려한 가중치로 평균을 냅니다

2.2 그래프 어텐션 신경망

- 그래프 어텐션 신경망(Graph Attention Network, GAT)에서는 가중치 자체도 학습합니다
- 실제 그래프에서는 이웃 별로 미치는 영향이 다를 수 있기 때문입니다
- 가중치를 학습하기 위해서 셀프-어텐션(Self-Attention)이 사용됩니다
- 각 층에서 정점 𝑖로부터 이웃 𝑗로의 가중치 𝜶𝒊𝒋는 세 단계를 통해 계산합니다
  1) 해당 층의 정점 𝑖의 임베딩 𝐡𝑖에 신경망𝑾를 곱해 새로운 임베딩을 얻습니다
  ![img](https://media.vlpt.us/images/skaurl/post/9b6d9197-7e8b-4ffa-966a-3fc55ba8eb62/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202021-02-26%2010.03.24.png)
  2) 정점 𝑖와 정점 𝑗의 새로운 임베딩을 연결한 후, 어텐션 계수 𝒂를 내적합니다 어텐션 계수 𝒂는 모든 정점이 공유하는 학습 변수입니다
  ![img](https://media.vlpt.us/images/skaurl/post/dc230da6-db9c-4f55-bd5a-63f876899005/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202021-02-26%2010.04.02.png)
  3) 2)의 결과에 소프트맥스(Softmax)를 적용합니다
  ![img](https://media.vlpt.us/images/skaurl/post/d22b4ff1-408c-4231-b689-0fd33c05f8a8/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202021-02-26%2010.04.26.png)
- 여러 개의 어텐션을 동시에 학습한 뒤, 결과를 연결하여 사용합니다
- 멀티헤드 어텐션(Multi-head Attention)이라고 부릅니다
- 어텐션의 결과 정점 분류의 정확도(Accuracy)가 향상되는 것을 확인할 수 있었습니다

#### 3. 그래프 표현 학습과 그래프 풀링

3.1 그래프 표현 학습

- 그래프 표현 학습, 혹은 그래프 임베딩이란 그래프 전체를 벡터의 형태로 표현하는 것입니다
- 개별 정점을 벡터의 형태로 표현하는 정점 표현 학습과 구분됩니다
- 그래프 임베딩은 벡터의 형태로 표현된 그래프 자체를 의미하기도 합니다
- 그래프 임베딩은 그래프 분류 등에 활용됩니다
- 그래프 형태로 표현된 화합물의 분자 구조로부터 특성을 예측하는 것이 한가지 예시입니다

3.2 그래프 풀링

- 그래프 풀링(Graph Pooling)이란 정점 임베딩들로부터 그래프 임베딩을 얻는 과정입니다
- 평균 등 단순한 방법보다 그래프의 구조를 고려한 방법을 사용할 경우, 그래프 분류 등의 후속 과제에서 더 높은 성능을 얻는 것으로 알려져 있습니다
- 미분가능한 풀링(Differentiable Pooling, DiffPool)은 군집 구조를 활용 임베딩을 계층적으로 집계합니다

#### 4. 지나친 획일화 문제

4.1 지나친 획일화 문제

- 지나친 획일화(Over-smoothing) 문제란 그래프 신경망의 층의 수가 증가하면서 정점의 임베딩이 서로 유사해지는 현상을 의미합니다
- 지나친 획일화 문제는 작은 세상 효과와 관련이 있습니다
- 적은 수의 층으로도 다수의 정점에 의해 영향을 받게 됩니다
- 지나친 획일화의 결과로 그래프 신경망의 층의 수를 늘렸을 때, 후속 과제에서의 정확도가 감소하는 현상이 발견되었습니다
- 그래프 신경망의 층이 2개 혹은 3개 일 때 정확도가 가장 높습니다
- 잔차항(Residual)을 넣는 것, 즉 이전 층의 임베딩을 한 번 더 더해주는 것 만으로는 효과가 제한적입니다

4.2 지나친 획일화 문제에 대한 대응

- 획일화 문제에 대한 대응으로 JK 네트워크(Jumping Knowledge Network)는 마지막 층의 임베딩 뿐 아니라, 모든 층의 임베딩을 함께 사용합니다
- APPNP는 0번째 층을 제외하고는 신경망 없이 집계 함수를 단순화하였습니다
- APPNP의 경우, 층의 수 증가에 따른 정확도 감소 효과가 없는 것을 확인했습니다

#### 5. 그래프 데이터의 증강

5.1 그래프 데이터 증강

- 데이터 증강(Data Augmentation)은 다양한 기계학습 문제에서 효과적입니다
- 그래프에도 누락되거나 부정확한 간선이 있을 수 있고, 데이터 증강을 통해 보완할 수 있습니다
- 임의 보행을 통해 정점간 유사도를 계산하고, 유사도가 높은 정점 간의 간선을 추가하는 방법이 제안되었습니다

5.2 그래프 데이터 증강의 효과

- 그래프 데이터 증강의 결과 정점 분류의 정확도가 개선되는 것을 확인했습니다

#### 10강 정리

1. 그래프 신경망 복습
2. 그래프 신경망에서의 어텐션
   - 그래프 어텐션 신경망은 이웃 정점들의 임베딩을 평균내는 과정에서의 가중치도 함께 학습함
3. 그래프 표현 학습과 그래프 풀링
   - 정점 임베딩으로부터 그래프 풀링을 통해 전체 그래프 임베딩, 즉 전체 그래프의 벡터 표현을 얻음
4. 지나친 획일화 문제
   - 그래프 신경망의 층 수를 증가시킬 때, 정점 임베딩이 서로 유사해지고, 후속 과제의 정확도가 떨어지는 현상
5. 그래프 데이터 증강
   - 그래프에 간선을 추가한 뒤 그래프 신경망을 학습시키는 방법으로 후속 과제의 정확도가 향상 시킴