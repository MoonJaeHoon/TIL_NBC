## monologg/koelectra-base-v3-discriminator

hidden_dim = 768

1. 생각보다 warmup_steps가 매우 중요한 역할을 하는 것을 확인하였다.
   - warmup steps를 적고 빠르게 가져가면서 scheduler로부터 lr 변화 주기를 많도록 했더니, 학습이 제대로 되지 않는 이슈가 발생하였다.
2. learning rate는 초반에는 너무 작게 가져갈 필요없이 1e-7 정도에서 warmup을 하며 1e-4를 max_lr로 설정하고 학습하는 것이 좋은듯 하다.
3. batch size는 24까지도 OOM 이 발생하지 않는다. (32로 설정했을 때에는 학습 도중에 OOM 발생)



## skplanet/dialog-koelectra-small-discriminator

hidden_dim = 128

1. 우선 장점으로는 업데이트할 parameter가 적어서 batch size를 64까지도 늘려서 학습할 수가 있었다.
2. 하지만, loss 관점에서 보면 10점대로 상대적으로 큰 값에서 학습을 시작하게 된다.
   - koelectra 모델은 9점대에서 시작하는 것을 고려하면 큰 차이라고 생각할 수 있을 것이다.
3. batch size를 상대적으로 크게 가져갈 수 있으므로 다양한 실험을 해봐야할 것 같다.



## monologg/kobert

1. 다른 모델에 비하여 학습속도 자체가 매우 빠른 편이다.
2. 하지만 큰 단점이 있는데, loss는 차츰 줄어가는 데 반하여, joint accuracy 결과가 0.01점대에서 계속 머물고 있다.
   - loss 역시 1 밑으로 내려가게끔 학습이 되질 않는 경향이 있다.
3. 학습속도 자체가 매우 빠르기 때문에 lr과 batch size를 적절하게 조정하여 joint accuracy를 끌어올릴 수만 있다면 매우 좋은 선택지가 될 수 있을 듯 하다.



## XLM-Roberta

hidden_dim = 1024

xlm-roberta 모델로부터의 subword_embedding을 사용해보려 했는데, train batch size == 8 에서 OOM 에러가 발생했다.

train batch size == 4 에서는 실행이 가능하긴 했지만 학습속도가 매우 느려 실험을 해볼 수 있는 여지가 없기 때문에 해당 모델은 사용하지 않는 것이 나아보였다.



- proj_dim - 768 , train_batch_size - 8=> OOM

- proj_dim - 128, train_batch_size - 24=> OOM

- proj_dim - 768 , train_batch_size - 4 => Possible



## CHAN

https://github.com/smartyfh/CHAN-DST