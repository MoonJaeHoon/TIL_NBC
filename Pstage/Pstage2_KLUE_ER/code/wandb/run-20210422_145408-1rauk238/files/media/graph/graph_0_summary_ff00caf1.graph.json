{"format": "torch", "nodes": [{"name": "roberta", "id": 139935328929104, "class_name": "RobertaModel(\n  (embeddings): RobertaEmbeddings(\n    (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n    (position_embeddings): Embedding(514, 1024, padding_idx=1)\n    (token_type_embeddings): Embedding(1, 1024)\n    (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): RobertaEncoder(\n    (layer): ModuleList(\n      (0): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (1): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (2): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (3): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (4): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (5): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (6): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (7): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (8): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (9): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (10): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (11): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (12): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (13): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (14): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (15): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (16): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (17): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (18): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (19): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (20): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (21): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (22): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (23): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=1024, out_features=1024, bias=True)\n            (key): Linear(in_features=1024, out_features=1024, bias=True)\n            (value): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n)", "parameters": [["embeddings.word_embeddings.weight", [250002, 1024]], ["embeddings.position_embeddings.weight", [514, 1024]], ["embeddings.token_type_embeddings.weight", [1, 1024]], ["embeddings.LayerNorm.weight", [1024]], ["embeddings.LayerNorm.bias", [1024]], ["encoder.layer.0.attention.self.query.weight", [1024, 1024]], ["encoder.layer.0.attention.self.query.bias", [1024]], ["encoder.layer.0.attention.self.key.weight", [1024, 1024]], ["encoder.layer.0.attention.self.key.bias", [1024]], ["encoder.layer.0.attention.self.value.weight", [1024, 1024]], ["encoder.layer.0.attention.self.value.bias", [1024]], ["encoder.layer.0.attention.output.dense.weight", [1024, 1024]], ["encoder.layer.0.attention.output.dense.bias", [1024]], ["encoder.layer.0.attention.output.LayerNorm.weight", [1024]], ["encoder.layer.0.attention.output.LayerNorm.bias", [1024]], ["encoder.layer.0.intermediate.dense.weight", [4096, 1024]], ["encoder.layer.0.intermediate.dense.bias", [4096]], ["encoder.layer.0.output.dense.weight", [1024, 4096]], ["encoder.layer.0.output.dense.bias", [1024]], ["encoder.layer.0.output.LayerNorm.weight", [1024]], ["encoder.layer.0.output.LayerNorm.bias", [1024]], ["encoder.layer.1.attention.self.query.weight", [1024, 1024]], ["encoder.layer.1.attention.self.query.bias", [1024]], ["encoder.layer.1.attention.self.key.weight", [1024, 1024]], ["encoder.layer.1.attention.self.key.bias", [1024]], ["encoder.layer.1.attention.self.value.weight", [1024, 1024]], ["encoder.layer.1.attention.self.value.bias", [1024]], ["encoder.layer.1.attention.output.dense.weight", [1024, 1024]], ["encoder.layer.1.attention.output.dense.bias", [1024]], ["encoder.layer.1.attention.output.LayerNorm.weight", [1024]], ["encoder.layer.1.attention.output.LayerNorm.bias", [1024]], ["encoder.layer.1.intermediate.dense.weight", [4096, 1024]], ["encoder.layer.1.intermediate.dense.bias", [4096]], ["encoder.layer.1.output.dense.weight", [1024, 4096]], ["encoder.layer.1.output.dense.bias", [1024]], ["encoder.layer.1.output.LayerNorm.weight", [1024]], ["encoder.layer.1.output.LayerNorm.bias", [1024]], ["encoder.layer.2.attention.self.query.weight", [1024, 1024]], ["encoder.layer.2.attention.self.query.bias", [1024]], ["encoder.layer.2.attention.self.key.weight", [1024, 1024]], ["encoder.layer.2.attention.self.key.bias", [1024]], ["encoder.layer.2.attention.self.value.weight", [1024, 1024]], ["encoder.layer.2.attention.self.value.bias", [1024]], ["encoder.layer.2.attention.output.dense.weight", [1024, 1024]], ["encoder.layer.2.attention.output.dense.bias", [1024]], ["encoder.layer.2.attention.output.LayerNorm.weight", [1024]], ["encoder.layer.2.attention.output.LayerNorm.bias", [1024]], ["encoder.layer.2.intermediate.dense.weight", [4096, 1024]], ["encoder.layer.2.intermediate.dense.bias", [4096]], ["encoder.layer.2.output.dense.weight", [1024, 4096]], ["encoder.layer.2.output.dense.bias", [1024]], ["encoder.layer.2.output.LayerNorm.weight", [1024]], ["encoder.layer.2.output.LayerNorm.bias", [1024]], ["encoder.layer.3.attention.self.query.weight", [1024, 1024]], ["encoder.layer.3.attention.self.query.bias", [1024]], ["encoder.layer.3.attention.self.key.weight", [1024, 1024]], ["encoder.layer.3.attention.self.key.bias", [1024]], ["encoder.layer.3.attention.self.value.weight", [1024, 1024]], ["encoder.layer.3.attention.self.value.bias", [1024]], ["encoder.layer.3.attention.output.dense.weight", [1024, 1024]], ["encoder.layer.3.attention.output.dense.bias", [1024]], ["encoder.layer.3.attention.output.LayerNorm.weight", [1024]], ["encoder.layer.3.attention.output.LayerNorm.bias", [1024]], ["encoder.layer.3.intermediate.dense.weight", [4096, 1024]], ["encoder.layer.3.intermediate.dense.bias", [4096]], ["encoder.layer.3.output.dense.weight", [1024, 4096]], ["encoder.layer.3.output.dense.bias", [1024]], ["encoder.layer.3.output.LayerNorm.weight", [1024]], ["encoder.layer.3.output.LayerNorm.bias", [1024]], ["encoder.layer.4.attention.self.query.weight", [1024, 1024]], ["encoder.layer.4.attention.self.query.bias", [1024]], ["encoder.layer.4.attention.self.key.weight", [1024, 1024]], ["encoder.layer.4.attention.self.key.bias", [1024]], ["encoder.layer.4.attention.self.value.weight", [1024, 1024]], ["encoder.layer.4.attention.self.value.bias", [1024]], ["encoder.layer.4.attention.output.dense.weight", [1024, 1024]], ["encoder.layer.4.attention.output.dense.bias", [1024]], ["encoder.layer.4.attention.output.LayerNorm.weight", [1024]], ["encoder.layer.4.attention.output.LayerNorm.bias", [1024]], ["encoder.layer.4.intermediate.dense.weight", [4096, 1024]], ["encoder.layer.4.intermediate.dense.bias", [4096]], ["encoder.layer.4.output.dense.weight", [1024, 4096]], ["encoder.layer.4.output.dense.bias", [1024]], ["encoder.layer.4.output.LayerNorm.weight", [1024]], ["encoder.layer.4.output.LayerNorm.bias", [1024]], ["encoder.layer.5.attention.self.query.weight", [1024, 1024]], ["encoder.layer.5.attention.self.query.bias", [1024]], ["encoder.layer.5.attention.self.key.weight", [1024, 1024]], ["encoder.layer.5.attention.self.key.bias", [1024]], ["encoder.layer.5.attention.self.value.weight", [1024, 1024]], ["encoder.layer.5.attention.self.value.bias", [1024]], ["encoder.layer.5.attention.output.dense.weight", [1024, 1024]], ["encoder.layer.5.attention.output.dense.bias", [1024]], ["encoder.layer.5.attention.output.LayerNorm.weight", [1024]], ["encoder.layer.5.attention.output.LayerNorm.bias", [1024]], ["encoder.layer.5.intermediate.dense.weight", [4096, 1024]], ["encoder.layer.5.intermediate.dense.bias", [4096]], ["encoder.layer.5.output.dense.weight", [1024, 4096]], ["encoder.layer.5.output.dense.bias", [1024]], ["encoder.layer.5.output.LayerNorm.weight", [1024]], ["encoder.layer.5.output.LayerNorm.bias", [1024]], ["encoder.layer.6.attention.self.query.weight", [1024, 1024]], ["encoder.layer.6.attention.self.query.bias", [1024]], ["encoder.layer.6.attention.self.key.weight", [1024, 1024]], ["encoder.layer.6.attention.self.key.bias", [1024]], ["encoder.layer.6.attention.self.value.weight", [1024, 1024]], ["encoder.layer.6.attention.self.value.bias", [1024]], ["encoder.layer.6.attention.output.dense.weight", [1024, 1024]], ["encoder.layer.6.attention.output.dense.bias", [1024]], ["encoder.layer.6.attention.output.LayerNorm.weight", [1024]], ["encoder.layer.6.attention.output.LayerNorm.bias", [1024]], ["encoder.layer.6.intermediate.dense.weight", [4096, 1024]], ["encoder.layer.6.intermediate.dense.bias", [4096]], ["encoder.layer.6.output.dense.weight", [1024, 4096]], ["encoder.layer.6.output.dense.bias", [1024]], ["encoder.layer.6.output.LayerNorm.weight", [1024]], ["encoder.layer.6.output.LayerNorm.bias", [1024]], ["encoder.layer.7.attention.self.query.weight", [1024, 1024]], ["encoder.layer.7.attention.self.query.bias", [1024]], ["encoder.layer.7.attention.self.key.weight", [1024, 1024]], ["encoder.layer.7.attention.self.key.bias", [1024]], ["encoder.layer.7.attention.self.value.weight", [1024, 1024]], ["encoder.layer.7.attention.self.value.bias", [1024]], ["encoder.layer.7.attention.output.dense.weight", [1024, 1024]], ["encoder.layer.7.attention.output.dense.bias", [1024]], ["encoder.layer.7.attention.output.LayerNorm.weight", [1024]], ["encoder.layer.7.attention.output.LayerNorm.bias", [1024]], ["encoder.layer.7.intermediate.dense.weight", [4096, 1024]], ["encoder.layer.7.intermediate.dense.bias", [4096]], ["encoder.layer.7.output.dense.weight", [1024, 4096]], ["encoder.layer.7.output.dense.bias", [1024]], ["encoder.layer.7.output.LayerNorm.weight", [1024]], ["encoder.layer.7.output.LayerNorm.bias", [1024]], ["encoder.layer.8.attention.self.query.weight", [1024, 1024]], ["encoder.layer.8.attention.self.query.bias", [1024]], ["encoder.layer.8.attention.self.key.weight", [1024, 1024]], ["encoder.layer.8.attention.self.key.bias", [1024]], ["encoder.layer.8.attention.self.value.weight", [1024, 1024]], ["encoder.layer.8.attention.self.value.bias", [1024]], ["encoder.layer.8.attention.output.dense.weight", [1024, 1024]], ["encoder.layer.8.attention.output.dense.bias", [1024]], ["encoder.layer.8.attention.output.LayerNorm.weight", [1024]], ["encoder.layer.8.attention.output.LayerNorm.bias", [1024]], ["encoder.layer.8.intermediate.dense.weight", [4096, 1024]], ["encoder.layer.8.intermediate.dense.bias", [4096]], ["encoder.layer.8.output.dense.weight", [1024, 4096]], ["encoder.layer.8.output.dense.bias", [1024]], ["encoder.layer.8.output.LayerNorm.weight", [1024]], ["encoder.layer.8.output.LayerNorm.bias", [1024]], ["encoder.layer.9.attention.self.query.weight", [1024, 1024]], ["encoder.layer.9.attention.self.query.bias", [1024]], ["encoder.layer.9.attention.self.key.weight", [1024, 1024]], ["encoder.layer.9.attention.self.key.bias", [1024]], ["encoder.layer.9.attention.self.value.weight", [1024, 1024]], ["encoder.layer.9.attention.self.value.bias", [1024]], ["encoder.layer.9.attention.output.dense.weight", [1024, 1024]], ["encoder.layer.9.attention.output.dense.bias", [1024]], ["encoder.layer.9.attention.output.LayerNorm.weight", [1024]], ["encoder.layer.9.attention.output.LayerNorm.bias", [1024]], ["encoder.layer.9.intermediate.dense.weight", [4096, 1024]], ["encoder.layer.9.intermediate.dense.bias", [4096]], ["encoder.layer.9.output.dense.weight", [1024, 4096]], ["encoder.layer.9.output.dense.bias", [1024]], ["encoder.layer.9.output.LayerNorm.weight", [1024]], ["encoder.layer.9.output.LayerNorm.bias", [1024]], ["encoder.layer.10.attention.self.query.weight", [1024, 1024]], ["encoder.layer.10.attention.self.query.bias", [1024]], ["encoder.layer.10.attention.self.key.weight", [1024, 1024]], ["encoder.layer.10.attention.self.key.bias", [1024]], ["encoder.layer.10.attention.self.value.weight", [1024, 1024]], ["encoder.layer.10.attention.self.value.bias", [1024]], ["encoder.layer.10.attention.output.dense.weight", [1024, 1024]], ["encoder.layer.10.attention.output.dense.bias", [1024]], ["encoder.layer.10.attention.output.LayerNorm.weight", [1024]], ["encoder.layer.10.attention.output.LayerNorm.bias", [1024]], ["encoder.layer.10.intermediate.dense.weight", [4096, 1024]], ["encoder.layer.10.intermediate.dense.bias", [4096]], ["encoder.layer.10.output.dense.weight", [1024, 4096]], ["encoder.layer.10.output.dense.bias", [1024]], ["encoder.layer.10.output.LayerNorm.weight", [1024]], ["encoder.layer.10.output.LayerNorm.bias", [1024]], ["encoder.layer.11.attention.self.query.weight", [1024, 1024]], ["encoder.layer.11.attention.self.query.bias", [1024]], ["encoder.layer.11.attention.self.key.weight", [1024, 1024]], ["encoder.layer.11.attention.self.key.bias", [1024]], ["encoder.layer.11.attention.self.value.weight", [1024, 1024]], ["encoder.layer.11.attention.self.value.bias", [1024]], ["encoder.layer.11.attention.output.dense.weight", [1024, 1024]], ["encoder.layer.11.attention.output.dense.bias", [1024]], ["encoder.layer.11.attention.output.LayerNorm.weight", [1024]], ["encoder.layer.11.attention.output.LayerNorm.bias", [1024]], ["encoder.layer.11.intermediate.dense.weight", [4096, 1024]], ["encoder.layer.11.intermediate.dense.bias", [4096]], ["encoder.layer.11.output.dense.weight", [1024, 4096]], ["encoder.layer.11.output.dense.bias", [1024]], ["encoder.layer.11.output.LayerNorm.weight", [1024]], ["encoder.layer.11.output.LayerNorm.bias", [1024]], ["encoder.layer.12.attention.self.query.weight", [1024, 1024]], ["encoder.layer.12.attention.self.query.bias", [1024]], ["encoder.layer.12.attention.self.key.weight", [1024, 1024]], ["encoder.layer.12.attention.self.key.bias", [1024]], ["encoder.layer.12.attention.self.value.weight", [1024, 1024]], ["encoder.layer.12.attention.self.value.bias", [1024]], ["encoder.layer.12.attention.output.dense.weight", [1024, 1024]], ["encoder.layer.12.attention.output.dense.bias", [1024]], ["encoder.layer.12.attention.output.LayerNorm.weight", [1024]], ["encoder.layer.12.attention.output.LayerNorm.bias", [1024]], ["encoder.layer.12.intermediate.dense.weight", [4096, 1024]], ["encoder.layer.12.intermediate.dense.bias", [4096]], ["encoder.layer.12.output.dense.weight", [1024, 4096]], ["encoder.layer.12.output.dense.bias", [1024]], ["encoder.layer.12.output.LayerNorm.weight", [1024]], ["encoder.layer.12.output.LayerNorm.bias", [1024]], ["encoder.layer.13.attention.self.query.weight", [1024, 1024]], ["encoder.layer.13.attention.self.query.bias", [1024]], ["encoder.layer.13.attention.self.key.weight", [1024, 1024]], ["encoder.layer.13.attention.self.key.bias", [1024]], ["encoder.layer.13.attention.self.value.weight", [1024, 1024]], ["encoder.layer.13.attention.self.value.bias", [1024]], ["encoder.layer.13.attention.output.dense.weight", [1024, 1024]], ["encoder.layer.13.attention.output.dense.bias", [1024]], ["encoder.layer.13.attention.output.LayerNorm.weight", [1024]], ["encoder.layer.13.attention.output.LayerNorm.bias", [1024]], ["encoder.layer.13.intermediate.dense.weight", [4096, 1024]], ["encoder.layer.13.intermediate.dense.bias", [4096]], ["encoder.layer.13.output.dense.weight", [1024, 4096]], ["encoder.layer.13.output.dense.bias", [1024]], ["encoder.layer.13.output.LayerNorm.weight", [1024]], ["encoder.layer.13.output.LayerNorm.bias", [1024]], ["encoder.layer.14.attention.self.query.weight", [1024, 1024]], ["encoder.layer.14.attention.self.query.bias", [1024]], ["encoder.layer.14.attention.self.key.weight", [1024, 1024]], ["encoder.layer.14.attention.self.key.bias", [1024]], ["encoder.layer.14.attention.self.value.weight", [1024, 1024]], ["encoder.layer.14.attention.self.value.bias", [1024]], ["encoder.layer.14.attention.output.dense.weight", [1024, 1024]], ["encoder.layer.14.attention.output.dense.bias", [1024]], ["encoder.layer.14.attention.output.LayerNorm.weight", [1024]], ["encoder.layer.14.attention.output.LayerNorm.bias", [1024]], ["encoder.layer.14.intermediate.dense.weight", [4096, 1024]], ["encoder.layer.14.intermediate.dense.bias", [4096]], ["encoder.layer.14.output.dense.weight", [1024, 4096]], ["encoder.layer.14.output.dense.bias", [1024]], ["encoder.layer.14.output.LayerNorm.weight", [1024]], ["encoder.layer.14.output.LayerNorm.bias", [1024]], ["encoder.layer.15.attention.self.query.weight", [1024, 1024]], ["encoder.layer.15.attention.self.query.bias", [1024]], ["encoder.layer.15.attention.self.key.weight", [1024, 1024]], ["encoder.layer.15.attention.self.key.bias", [1024]], ["encoder.layer.15.attention.self.value.weight", [1024, 1024]], ["encoder.layer.15.attention.self.value.bias", [1024]], ["encoder.layer.15.attention.output.dense.weight", [1024, 1024]], ["encoder.layer.15.attention.output.dense.bias", [1024]], ["encoder.layer.15.attention.output.LayerNorm.weight", [1024]], ["encoder.layer.15.attention.output.LayerNorm.bias", [1024]], ["encoder.layer.15.intermediate.dense.weight", [4096, 1024]], ["encoder.layer.15.intermediate.dense.bias", [4096]], ["encoder.layer.15.output.dense.weight", [1024, 4096]], ["encoder.layer.15.output.dense.bias", [1024]], ["encoder.layer.15.output.LayerNorm.weight", [1024]], ["encoder.layer.15.output.LayerNorm.bias", [1024]], ["encoder.layer.16.attention.self.query.weight", [1024, 1024]], ["encoder.layer.16.attention.self.query.bias", [1024]], ["encoder.layer.16.attention.self.key.weight", [1024, 1024]], ["encoder.layer.16.attention.self.key.bias", [1024]], ["encoder.layer.16.attention.self.value.weight", [1024, 1024]], ["encoder.layer.16.attention.self.value.bias", [1024]], ["encoder.layer.16.attention.output.dense.weight", [1024, 1024]], ["encoder.layer.16.attention.output.dense.bias", [1024]], ["encoder.layer.16.attention.output.LayerNorm.weight", [1024]], ["encoder.layer.16.attention.output.LayerNorm.bias", [1024]], ["encoder.layer.16.intermediate.dense.weight", [4096, 1024]], ["encoder.layer.16.intermediate.dense.bias", [4096]], ["encoder.layer.16.output.dense.weight", [1024, 4096]], ["encoder.layer.16.output.dense.bias", [1024]], ["encoder.layer.16.output.LayerNorm.weight", [1024]], ["encoder.layer.16.output.LayerNorm.bias", [1024]], ["encoder.layer.17.attention.self.query.weight", [1024, 1024]], ["encoder.layer.17.attention.self.query.bias", [1024]], ["encoder.layer.17.attention.self.key.weight", [1024, 1024]], ["encoder.layer.17.attention.self.key.bias", [1024]], ["encoder.layer.17.attention.self.value.weight", [1024, 1024]], ["encoder.layer.17.attention.self.value.bias", [1024]], ["encoder.layer.17.attention.output.dense.weight", [1024, 1024]], ["encoder.layer.17.attention.output.dense.bias", [1024]], ["encoder.layer.17.attention.output.LayerNorm.weight", [1024]], ["encoder.layer.17.attention.output.LayerNorm.bias", [1024]], ["encoder.layer.17.intermediate.dense.weight", [4096, 1024]], ["encoder.layer.17.intermediate.dense.bias", [4096]], ["encoder.layer.17.output.dense.weight", [1024, 4096]], ["encoder.layer.17.output.dense.bias", [1024]], ["encoder.layer.17.output.LayerNorm.weight", [1024]], ["encoder.layer.17.output.LayerNorm.bias", [1024]], ["encoder.layer.18.attention.self.query.weight", [1024, 1024]], ["encoder.layer.18.attention.self.query.bias", [1024]], ["encoder.layer.18.attention.self.key.weight", [1024, 1024]], ["encoder.layer.18.attention.self.key.bias", [1024]], ["encoder.layer.18.attention.self.value.weight", [1024, 1024]], ["encoder.layer.18.attention.self.value.bias", [1024]], ["encoder.layer.18.attention.output.dense.weight", [1024, 1024]], ["encoder.layer.18.attention.output.dense.bias", [1024]], ["encoder.layer.18.attention.output.LayerNorm.weight", [1024]], ["encoder.layer.18.attention.output.LayerNorm.bias", [1024]], ["encoder.layer.18.intermediate.dense.weight", [4096, 1024]], ["encoder.layer.18.intermediate.dense.bias", [4096]], ["encoder.layer.18.output.dense.weight", [1024, 4096]], ["encoder.layer.18.output.dense.bias", [1024]], ["encoder.layer.18.output.LayerNorm.weight", [1024]], ["encoder.layer.18.output.LayerNorm.bias", [1024]], ["encoder.layer.19.attention.self.query.weight", [1024, 1024]], ["encoder.layer.19.attention.self.query.bias", [1024]], ["encoder.layer.19.attention.self.key.weight", [1024, 1024]], ["encoder.layer.19.attention.self.key.bias", [1024]], ["encoder.layer.19.attention.self.value.weight", [1024, 1024]], ["encoder.layer.19.attention.self.value.bias", [1024]], ["encoder.layer.19.attention.output.dense.weight", [1024, 1024]], ["encoder.layer.19.attention.output.dense.bias", [1024]], ["encoder.layer.19.attention.output.LayerNorm.weight", [1024]], ["encoder.layer.19.attention.output.LayerNorm.bias", [1024]], ["encoder.layer.19.intermediate.dense.weight", [4096, 1024]], ["encoder.layer.19.intermediate.dense.bias", [4096]], ["encoder.layer.19.output.dense.weight", [1024, 4096]], ["encoder.layer.19.output.dense.bias", [1024]], ["encoder.layer.19.output.LayerNorm.weight", [1024]], ["encoder.layer.19.output.LayerNorm.bias", [1024]], ["encoder.layer.20.attention.self.query.weight", [1024, 1024]], ["encoder.layer.20.attention.self.query.bias", [1024]], ["encoder.layer.20.attention.self.key.weight", [1024, 1024]], ["encoder.layer.20.attention.self.key.bias", [1024]], ["encoder.layer.20.attention.self.value.weight", [1024, 1024]], ["encoder.layer.20.attention.self.value.bias", [1024]], ["encoder.layer.20.attention.output.dense.weight", [1024, 1024]], ["encoder.layer.20.attention.output.dense.bias", [1024]], ["encoder.layer.20.attention.output.LayerNorm.weight", [1024]], ["encoder.layer.20.attention.output.LayerNorm.bias", [1024]], ["encoder.layer.20.intermediate.dense.weight", [4096, 1024]], ["encoder.layer.20.intermediate.dense.bias", [4096]], ["encoder.layer.20.output.dense.weight", [1024, 4096]], ["encoder.layer.20.output.dense.bias", [1024]], ["encoder.layer.20.output.LayerNorm.weight", [1024]], ["encoder.layer.20.output.LayerNorm.bias", [1024]], ["encoder.layer.21.attention.self.query.weight", [1024, 1024]], ["encoder.layer.21.attention.self.query.bias", [1024]], ["encoder.layer.21.attention.self.key.weight", [1024, 1024]], ["encoder.layer.21.attention.self.key.bias", [1024]], ["encoder.layer.21.attention.self.value.weight", [1024, 1024]], ["encoder.layer.21.attention.self.value.bias", [1024]], ["encoder.layer.21.attention.output.dense.weight", [1024, 1024]], ["encoder.layer.21.attention.output.dense.bias", [1024]], ["encoder.layer.21.attention.output.LayerNorm.weight", [1024]], ["encoder.layer.21.attention.output.LayerNorm.bias", [1024]], ["encoder.layer.21.intermediate.dense.weight", [4096, 1024]], ["encoder.layer.21.intermediate.dense.bias", [4096]], ["encoder.layer.21.output.dense.weight", [1024, 4096]], ["encoder.layer.21.output.dense.bias", [1024]], ["encoder.layer.21.output.LayerNorm.weight", [1024]], ["encoder.layer.21.output.LayerNorm.bias", [1024]], ["encoder.layer.22.attention.self.query.weight", [1024, 1024]], ["encoder.layer.22.attention.self.query.bias", [1024]], ["encoder.layer.22.attention.self.key.weight", [1024, 1024]], ["encoder.layer.22.attention.self.key.bias", [1024]], ["encoder.layer.22.attention.self.value.weight", [1024, 1024]], ["encoder.layer.22.attention.self.value.bias", [1024]], ["encoder.layer.22.attention.output.dense.weight", [1024, 1024]], ["encoder.layer.22.attention.output.dense.bias", [1024]], ["encoder.layer.22.attention.output.LayerNorm.weight", [1024]], ["encoder.layer.22.attention.output.LayerNorm.bias", [1024]], ["encoder.layer.22.intermediate.dense.weight", [4096, 1024]], ["encoder.layer.22.intermediate.dense.bias", [4096]], ["encoder.layer.22.output.dense.weight", [1024, 4096]], ["encoder.layer.22.output.dense.bias", [1024]], ["encoder.layer.22.output.LayerNorm.weight", [1024]], ["encoder.layer.22.output.LayerNorm.bias", [1024]], ["encoder.layer.23.attention.self.query.weight", [1024, 1024]], ["encoder.layer.23.attention.self.query.bias", [1024]], ["encoder.layer.23.attention.self.key.weight", [1024, 1024]], ["encoder.layer.23.attention.self.key.bias", [1024]], ["encoder.layer.23.attention.self.value.weight", [1024, 1024]], ["encoder.layer.23.attention.self.value.bias", [1024]], ["encoder.layer.23.attention.output.dense.weight", [1024, 1024]], ["encoder.layer.23.attention.output.dense.bias", [1024]], ["encoder.layer.23.attention.output.LayerNorm.weight", [1024]], ["encoder.layer.23.attention.output.LayerNorm.bias", [1024]], ["encoder.layer.23.intermediate.dense.weight", [4096, 1024]], ["encoder.layer.23.intermediate.dense.bias", [4096]], ["encoder.layer.23.output.dense.weight", [1024, 4096]], ["encoder.layer.23.output.dense.bias", [1024]], ["encoder.layer.23.output.LayerNorm.weight", [1024]], ["encoder.layer.23.output.LayerNorm.bias", [1024]]], "output_shape": [[[[0], [0], [0], [0], [0], [0], [0], [0], 0, [0], [0], 0, 0, 0, 0, 0, 0]]], "num_parameters": [256002048, 526336, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1048576, 1024, 1024, 1024, 4194304, 4096, 4194304, 1024, 1024, 1024]}, {"name": "classifier", "id": 139935350010384, "class_name": "RobertaClassificationHead(\n  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n  (out_proj): Linear(in_features=1024, out_features=42, bias=True)\n)", "parameters": [["dense.weight", [1024, 1024]], ["dense.bias", [1024]], ["out_proj.weight", [42, 1024]], ["out_proj.bias", [42]]], "output_shape": [[32, 42]], "num_parameters": [1048576, 1024, 43008, 42]}], "edges": []}