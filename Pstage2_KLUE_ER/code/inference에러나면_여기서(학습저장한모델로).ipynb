{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, BertConfig, AutoModelForSequenceClassification, AutoConfig\n",
    "from transformers import AdamW, get_constant_schedule, get_constant_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "from load_data import *\n",
    "import argparse\n",
    "from importlib import import_module\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import re\n",
    "import neptune\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from torch.utils.data import DataLoader\n",
    "import inference\n",
    "\n",
    "def inference_proba(model, tokenized_sent, device):\n",
    "  dataloader = DataLoader(tokenized_sent, batch_size=8, shuffle=False)\n",
    "  model.eval()\n",
    "  output_pred = []\n",
    "  result=np.zeros(42).reshape(-1,42)\n",
    "  device=torch.device('cuda')\n",
    "  # device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "  for i, data in enumerate(dataloader):\n",
    "    with torch.no_grad():\n",
    "      # outputs = model(\n",
    "      #     input_ids=data['input_ids'].to(device),\n",
    "      #     attention_mask=data['attention_mask'].to(device),\n",
    "      #     token_type_ids=data['token_type_ids'].to(device)\n",
    "      #     )\n",
    "      outputs = model(\n",
    "          input_ids=data['input_ids'],\n",
    "          attention_mask=data['attention_mask'],\n",
    "          token_type_ids=data['token_type_ids']\n",
    "          )\n",
    "    logits = outputs[0]\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    # result = np.argmax(logits, axis=-1)\n",
    "    result=np.vstack([result,logits])\n",
    "\n",
    "  return result[1:,:]\n",
    "\n",
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "##################################################################\n",
    "def preprocessing_dataset(dataset, label_type, apply_add_entity):\n",
    "  label = []\n",
    "  for i in dataset.iloc[:,8]:\n",
    "    if i == 'blind':\n",
    "      label.append(100)\n",
    "    else:\n",
    "      label.append(label_type[i])\n",
    "  out_dataset = pd.DataFrame({'sentence':dataset.iloc[:,1],'entity_01':dataset.iloc[:,2],'entity_02':dataset.iloc[:,5],'label':label,})\n",
    "  if apply_add_entity == True:\n",
    "    out_dataset['sentence'] = out_dataset.apply(lambda x: x['entity_01']+\"[SEP]\"+x['entity_02']+\"[SEP]\"+x['sentence'], axis=1)\n",
    "  return out_dataset\n",
    "\n",
    "def load_test_dataset(dataset_dir, tokenizer):\n",
    "  test_dataset = load_test(dataset_dir)\n",
    "\n",
    "  test_label = test_dataset['label'].values\n",
    "  # tokenizing dataset\n",
    "  tokenized_test = tokenized_dataset(test_dataset, tokenizer)\n",
    "  return tokenized_test, test_label\n",
    "\n",
    "def lower_dir_search(dirname):  # 모델저장된 체크포인트 폴더 경로들을 찾아주는 함수\n",
    "  filenames = os.listdir(dirname)\n",
    "  lower_dir_list = []\n",
    "  for filename in filenames:\n",
    "      full_filename = os.path.join(dirname, filename)\n",
    "      lower_dir_list.append(full_filename)\n",
    "  return lower_dir_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그때그때마다 다르게 지정해줘야할 파라미터들"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KFOLD 모델 말고 단순 모델\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file saved!!\n",
      "../submission/fold_중_한개_xlm-roberta-large5_5/fold_중_한개_xlm-roberta-large5_5.csv\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "seed_everything(77)\n",
    "# model_type = 'Electra'\n",
    "model_type = 'XLMRoberta'\n",
    "# MODEL_NAME = 'monologg/koelectra-base-v3-discriminator'\n",
    "MODEL_NAME = 'xlm-roberta-large'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "upper_dir = f\"kfold_results/{MODEL_NAME.replace('/','_')}\" + '5'  #모델 폴드의 상위경로\n",
    "upper_dir = upper_dir + '/' + '5' # 몇번째 폴드인지 \n",
    "model_path = upper_dir+'fold'\n",
    "max_len = 128\n",
    "apply_add_entity=False\n",
    "\n",
    "def tokenized_dataset(dataset, tokenizer):\n",
    "  concat_entity = []\n",
    "  if 'Roberta' in model_type:\n",
    "    for e01, e02 in zip(dataset['entity_01'], dataset['entity_02']):\n",
    "      temp = ''\n",
    "      temp = '<s>' + e01 + '</s>' + '<s>' + e02 + '</s>'\n",
    "      concat_entity.append(temp)\n",
    "  else:\n",
    "    for e01, e02 in zip(dataset['entity_01'], dataset['entity_02']):\n",
    "      temp = ''\n",
    "      temp = e01 + '[SEP]' + e02\n",
    "      concat_entity.append(temp)\n",
    "  tokenized_sentences = tokenizer(\n",
    "      concat_entity,\n",
    "      list(dataset['sentence']),\n",
    "      return_tensors=\"pt\",\n",
    "      padding=True,\n",
    "      truncation=True,\n",
    "      # max_length=100,\n",
    "      max_length=max_len,\n",
    "\n",
    "      add_special_tokens=True,\n",
    "      )\n",
    "  return tokenized_sentences\n",
    "\n",
    "def load_test(dataset_dir):\n",
    "  # load label_type, classes\n",
    "  with open('../input/data/label_type.pkl', 'rb') as f:\n",
    "    label_type = pickle.load(f)\n",
    "  # load dataset\n",
    "  dataset = pd.read_csv(dataset_dir, delimiter='\\t', header=None)\n",
    "  # preprecessing dataset\n",
    "  dataset = preprocessing_dataset(dataset, label_type,apply_add_entity=apply_add_entity)\n",
    "  \n",
    "  return dataset\n",
    "\n",
    "def inference_proba(model, tokenized_sent, device):\n",
    "  dataloader = DataLoader(tokenized_sent, batch_size=40, shuffle=False)\n",
    "  model.eval()\n",
    "  output_pred = []\n",
    "  result=np.zeros(42).reshape(-1,42)\n",
    "\n",
    "  for i, data in enumerate(dataloader):\n",
    "    with torch.no_grad():\n",
    "      if 'Roberta' in model_type:\n",
    "        outputs = model(\n",
    "            input_ids=data['input_ids'],\n",
    "            attention_mask=data['attention_mask'],\n",
    "            # token_type_ids=data['token_type_ids']\n",
    "            )\n",
    "      else:\n",
    "        outputs = model(\n",
    "            input_ids=data['input_ids'],\n",
    "            attention_mask=data['attention_mask'],\n",
    "            token_type_ids=data['token_type_ids']\n",
    "            )\n",
    "      # outputs = model(\n",
    "      #     input_ids=data['input_ids'].to(device),\n",
    "      #     attention_mask=data['attention_mask'].to(device),\n",
    "      #     token_type_ids=data['token_type_ids'].to(device)\n",
    "      #     )\n",
    "    logits = outputs[0]\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    # result = np.argmax(logits, axis=-1)\n",
    "    result=np.vstack([result,logits])\n",
    "\n",
    "  return result[1:,:]\n",
    "\n",
    "def inference(model, tokenized_sent, device):\n",
    "  dataloader = DataLoader(tokenized_sent, batch_size=40, shuffle=False)\n",
    "  model.eval()\n",
    "  output_pred = []\n",
    "  \n",
    "  for i, data in enumerate(dataloader):\n",
    "    with torch.no_grad():\n",
    "      if 'Roberta' in model_type:\n",
    "        outputs = model(\n",
    "            input_ids=data['input_ids'],\n",
    "            attention_mask=data['attention_mask'],\n",
    "            # token_type_ids=data['token_type_ids']\n",
    "            )\n",
    "      else:\n",
    "        outputs = model(\n",
    "            input_ids=data['input_ids'],\n",
    "            attention_mask=data['attention_mask'],\n",
    "            token_type_ids=data['token_type_ids']\n",
    "            )\n",
    "      # outputs = model(\n",
    "      #     input_ids=data['input_ids'].to(device),\n",
    "      #     attention_mask=data['attention_mask'].to(device),\n",
    "      #     token_type_ids=data['token_type_ids'].to(device)\n",
    "      #     )\n",
    "    logits = outputs[0]\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    result = np.argmax(logits, axis=-1)\n",
    "    output_pred.append(result)\n",
    "  from itertools import chain\n",
    "  output_pred = list(chain(*output_pred))\n",
    "\n",
    "  return np.array(output_pred).flatten()\n",
    "\n",
    "\n",
    "test_dataset_dir = \"../input/data/test/test.tsv\"\n",
    "test_dataset, test_label = load_test_dataset(test_dataset_dir, tokenizer)\n",
    "test_dataset = RE_Dataset(test_dataset ,test_label)\n",
    "\n",
    "lower_dir_list = lower_dir_search(f'{model_path}')  # 저장된 모든 모델들의 checkpoint 폴더경로 리스트\n",
    "lower_dir_list = [lower_dir for lower_dir in lower_dir_list if 'checkpoint-' in lower_dir]\n",
    "maximum_check_dir = sorted(lower_dir_list, key=lambda x: int(re.search(rf\"checkpoint\\-[0-9]+\",x).group().replace(\"checkpoint-\",\"\")))[-1]\n",
    "model_module = getattr(import_module(\"transformers\"), f\"{model_type}\" + \"ForSequenceClassification\")\n",
    "model = model_module.from_pretrained(maximum_check_dir)\n",
    "# device = torch.device('cuda')\n",
    "# device = torch.device('cpu')\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "pred_answer = inference(model, test_dataset, device)\n",
    "\n",
    "output = pd.DataFrame(pred_answer, columns=['pred'])\n",
    "submission_folder_path = f\"../submission/fold_중_한개_{upper_dir.split('/')[-2]}_{upper_dir.split('/')[-1]}/\"\n",
    "if not os.path.isdir(submission_folder_path):\n",
    "  os.makedirs(submission_folder_path)\n",
    "inference_file_path = submission_folder_path + f\"fold_중_한개_{upper_dir.split('/')[-2]}_{upper_dir.split('/')[-1]}.csv\"\n",
    "\n",
    "output.to_csv(inference_file_path, index=False)\n",
    "print('Submission file saved!!')\n",
    "print(inference_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use KFOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "seed_everything(77)\n",
    "# model_type = 'Bert'\n",
    "# model_type = 'Electra'\n",
    "model_type = 'XLMRoberta'\n",
    "# MODEL_NAME = 'bert-base-multilingual-cased'\n",
    "# MODEL_NAME = 'monologg/koelectra-base-v3-discriminator'\n",
    "MODEL_NAME = 'xlm-roberta-large'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "upper_dir = f\"kfold_results/{MODEL_NAME.replace('/','_')}\" + '6'\n",
    "max_len = 128\n",
    "apply_add_entity=False\n",
    "num_fold_k = 6  # 6인데.. 공간없어서 못하는중\n",
    "use_kfold=True\n",
    "\n",
    "def tokenized_dataset(dataset, tokenizer):\n",
    "  concat_entity = []\n",
    "  if 'Roberta' in model_type:\n",
    "    for e01, e02 in zip(dataset['entity_01'], dataset['entity_02']):\n",
    "      temp = ''\n",
    "      temp = '<s>' + e01 + '</s>' + '<s>' + e02 + '</s>'\n",
    "      concat_entity.append(temp)\n",
    "  else:\n",
    "    for e01, e02 in zip(dataset['entity_01'], dataset['entity_02']):\n",
    "      temp = ''\n",
    "      temp = e01 + '[SEP]' + e02\n",
    "      concat_entity.append(temp)\n",
    "  tokenized_sentences = tokenizer(\n",
    "      concat_entity,\n",
    "      list(dataset['sentence']),\n",
    "      return_tensors=\"pt\",\n",
    "      padding=True,\n",
    "      truncation=True,\n",
    "      # max_length=100,\n",
    "      max_length=max_len,\n",
    "\n",
    "      add_special_tokens=True,\n",
    "      )\n",
    "  return tokenized_sentences\n",
    "\n",
    "def load_test(dataset_dir):\n",
    "  # load label_type, classes\n",
    "  with open('../input/data/label_type.pkl', 'rb') as f:\n",
    "    label_type = pickle.load(f)\n",
    "  # load dataset\n",
    "  dataset = pd.read_csv(dataset_dir, delimiter='\\t')\n",
    "  # preprecessing dataset\n",
    "  dataset = preprocessing_dataset(dataset, label_type,apply_add_entity=apply_add_entity)\n",
    "  \n",
    "  return dataset\n",
    "\n",
    "def inference_proba(model, tokenized_sent, device):\n",
    "  dataloader = DataLoader(tokenized_sent, batch_size=40, shuffle=False)\n",
    "  model.eval()\n",
    "  output_pred = []\n",
    "  result=np.zeros(42).reshape(-1,42)\n",
    "\n",
    "  for i, data in enumerate(dataloader):\n",
    "    with torch.no_grad():\n",
    "      if 'Roberta' in model_type:\n",
    "        outputs = model(\n",
    "            input_ids=data['input_ids'],\n",
    "            attention_mask=data['attention_mask'],\n",
    "            # token_type_ids=data['token_type_ids']\n",
    "            )\n",
    "      else:\n",
    "        outputs = model(\n",
    "            input_ids=data['input_ids'],\n",
    "            attention_mask=data['attention_mask'],\n",
    "            token_type_ids=data['token_type_ids']\n",
    "            )\n",
    "      # outputs = model(\n",
    "      #     input_ids=data['input_ids'].to(device),\n",
    "      #     attention_mask=data['attention_mask'].to(device),\n",
    "      #     token_type_ids=data['token_type_ids'].to(device)\n",
    "      #     )\n",
    "    logits = outputs[0]\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    # result = np.argmax(logits, axis=-1)\n",
    "    result=np.vstack([result,logits])\n",
    "\n",
    "  return result[1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset_dir = \"../input/data/test/test.tsv\"\n",
    "test_dataset_dir = \"../input/data/test/final_test_ner.tsv\"\n",
    "test_dataset, test_label = load_test_dataset(test_dataset_dir, tokenizer)\n",
    "test_dataset = RE_Dataset(test_dataset ,test_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [11:09, 365.72s/it]"
     ]
    }
   ],
   "source": [
    "assert (use_kfold==True)\n",
    "from tqdm import tqdm\n",
    "predictions_of_proba = []\n",
    "\n",
    "k_range = range(num_fold_k)\n",
    "# k_range = [1,4,5]\n",
    "# k_range = [1,5]\n",
    "# k_range = [1,2,4,5]\n",
    "# k_range = [1,4,5]\n",
    "# weight = [0.4,0.3,0.3]\n",
    "\n",
    "# for k_idx,weigh in tqdm(zip(k_range,weight)):\n",
    "for k_idx in tqdm(k_range):\n",
    "  model_path = upper_dir+f'/{k_idx}fold'\n",
    "  lower_dir_list = lower_dir_search(f'{model_path}')  # 저장된 모든 모델들의 checkpoint 폴더경로 리스트\n",
    "  lower_dir_list = [lower_dir for lower_dir in lower_dir_list if 'checkpoint-' in lower_dir]\n",
    "  maximum_check_dir = sorted(lower_dir_list, key=lambda x: int(re.search(rf\"checkpoint\\-[0-9]+\",x).group().replace(\"checkpoint-\",\"\")))[-1]\n",
    "  model_module = getattr(import_module(\"transformers\"), f\"{model_type}\" + \"ForSequenceClassification\")\n",
    "  model = model_module.from_pretrained(maximum_check_dir)\n",
    "  # device = torch.device('cuda')\n",
    "  pred_answer = inference_proba(model, test_dataset, device)\n",
    "  # predictions_of_proba.append(weigh*pred_answer)\n",
    "  predictions_of_proba.append(pred_answer)\n",
    "\n",
    "mean_ensemble_result = np.argmax(np.mean(predictions_of_proba,axis=0),axis=1)  # 평균 앙상블\n",
    "output = pd.DataFrame(mean_ensemble_result, columns=['pred'])\n",
    "submission_folder_path = f\"../submission/{num_fold_k}fold_{model_path.split('/')[-2]}/\"\n",
    "if not os.path.isdir(submission_folder_path):\n",
    "  os.makedirs(submission_folder_path)\n",
    "inference_file_path = submission_folder_path + f\"{num_fold_k}fold_{model_path.split('/')[-2]}.csv\"\n",
    "\n",
    "output.to_csv(inference_file_path, index=False)\n",
    "print('Submission file saved!!')\n",
    "print(inference_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inference_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proba 따로 해서 저장해놓기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "xlm-roberta-large7\n",
      "(1000, 42)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "seed_everything(77)\n",
    "# model_type = 'Bert'\n",
    "# model_type = 'Electra'\n",
    "model_type = 'XLMRoberta'\n",
    "# MODEL_NAME = 'bert-base-multilingual-cased'\n",
    "# MODEL_NAME = 'monologg/koelectra-base-v3-discriminator'\n",
    "MODEL_NAME = 'xlm-roberta-large'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "upper_dir = f\"kfold_results/{MODEL_NAME.replace('/','_')}\" + '7'\n",
    "max_len = 128\n",
    "apply_add_entity=False\n",
    "# num_fold_k = 6  # 6인데.. 공간없어서 못하는중\n",
    "# use_kfold=True\n",
    "\n",
    "k_idx = 0 # 0번째 폴드\n",
    "k_idx = 1 # 1번째 폴드\n",
    "k_idx = 2 # 1번째 폴드\n",
    "k_idx = 3 # 1번째 폴드\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "predictions_of_proba = []\n",
    "\n",
    "model_path = upper_dir+f'/{k_idx}fold'\n",
    "lower_dir_list = lower_dir_search(f'{model_path}')  # 저장된 모든 모델들의 checkpoint 폴더경로 리스트\n",
    "lower_dir_list = [lower_dir for lower_dir in lower_dir_list if 'checkpoint-' in lower_dir]\n",
    "maximum_check_dir = sorted(lower_dir_list, key=lambda x: int(re.search(rf\"checkpoint\\-[0-9]+\",x).group().replace(\"checkpoint-\",\"\")))[-1]\n",
    "model_module = getattr(import_module(\"transformers\"), f\"{model_type}\" + \"ForSequenceClassification\")\n",
    "model = model_module.from_pretrained(maximum_check_dir)\n",
    "# device = torch.device('cuda')\n",
    "pred_answer = inference_proba(model, test_dataset, device)\n",
    "print(type(pred_answer))\n",
    "print(upper_dir.split('/')[-1])\n",
    "print(pred_answer.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.5046, -0.0341,  3.8955,  ..., -0.3998, -0.2050, -0.4232],\n        [-0.5850,  0.0099, -0.0280,  ...,  0.0060, -0.0199, -0.1700],\n        [-0.2243,  2.3287,  0.3073,  ...,  0.2171, -0.0744, -0.3788],\n        ...,\n        [-0.3803, -0.0639,  0.0592,  ...,  0.0647, -0.0286, -0.0361],\n        [ 3.8256, -0.0714, -0.4178,  ..., -0.1105, -0.0479,  0.0083],\n        [ 0.7290, -0.0861, -0.1932,  ...,  0.0615, -0.0532, -0.2908]],\n       device='cuda:0', dtype=torch.float64)"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# - .npy 파일로 로컬에 저장\n",
    "# numpy 배열 'pred_answer'를 경로 '/data/npyfile.npy'로 저장\n",
    "\n",
    "npy_save_folder = f\"../for_submission/{upper_dir.split('/')[-1]}\"\n",
    "if not os.path.isdir(npy_save_folder):\n",
    "  os.makedirs(npy_save_folder)\n",
    "\n",
    "npy_save_dir = f'{npy_save_folder}/npy_file_{k_idx}'\n",
    "\n",
    "np.save(npy_save_dir, pred_answer)\n",
    "\n",
    "# - .npy 파일 불러오기\n",
    "# np_load.shape == (3, 50, 50)\n",
    "\n",
    "np_load = np.load(f'{npy_save_dir}.npy')\n",
    "\n",
    "# - numpy Array -> Tensor\n",
    "# device type이 'cuda'인 tensor 완성\n",
    "\n",
    "load_tensor = torch.from_numpy(np_load).to('cuda')\n",
    "load_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file saved!!\n",
      "../submission/4fold_xlm-roberta-large7/4fold_xlm-roberta-large7.csv\n"
     ]
    }
   ],
   "source": [
    "## 모아둔 numpy 결과파일들 앙상블하기\n",
    "\n",
    "upper_dir = f\"kfold_results/{MODEL_NAME.replace('/','_')}\" + '7'\n",
    "npy_save_folder = f\"../for_submission/{upper_dir.split('/')[-1]}\"\n",
    "# npy_save_dir = f'{npy_save_folder}/npy_file_{k_idx}'\n",
    "num_fold_k=4\n",
    "predictions_of_proba = []\n",
    "for idx in range(num_fold_k):\n",
    "  pred_answer = np.load(f'{npy_save_folder}/npy_file_{idx}.npy')\n",
    "  predictions_of_proba.append(pred_answer)\n",
    "\n",
    "mean_ensemble_result = np.argmax(np.mean(predictions_of_proba,axis=0),axis=1)  # 평균 앙상블\n",
    "output = pd.DataFrame(mean_ensemble_result, columns=['pred'])\n",
    "submission_folder_path = f\"../submission/{num_fold_k}fold_{npy_save_folder.split('/')[-1]}/\"\n",
    "if not os.path.isdir(submission_folder_path):\n",
    "  os.makedirs(submission_folder_path)\n",
    "inference_file_path = submission_folder_path + f\"{num_fold_k}fold_{npy_save_folder.split('/')[-1]}.csv\"\n",
    "\n",
    "output.to_csv(inference_file_path, index=False)\n",
    "print('Submission file saved!!')\n",
    "print(inference_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('conda': virtualenv)",
   "language": "python",
   "name": "python377jvsc74a57bd098b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}